[strings]
# Mode : train, test, serve
mode = train
#seq_data = train_data/seq.data
seq_data = train_data/Eng_conv/save.csv
train_data=train_data
#训练集原始文件
#resource_data = train_data/xiaohuangji50w_nofenci.conv
emb_file = vectors/glove.6B.100d.txt
emo_emb_file=vectors/emotion.100d.txt
resource_data = train_data/Eng_conv/train.csv
#读取识别原始文件中段落和行头的标示
emo_input=self_att


model_data = model_data

[ints]
# vocabulary size 
# 	20,000 is a reasonable size
enc_vocab_size = 20000
dec_vocab_size = 20000
embedding_dim=128
emp_embedding_dim=100

# typical options : 128, 256, 512, 1024
layer_size = 256
emp_layer_size = 200
# dataset size limit; typically none : no limit
max_train_data_size = 84169
#batch_size = 32
batch_size = 448


